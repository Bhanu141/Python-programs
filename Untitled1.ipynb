{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQNuNqYpE7xNcksytqAk2f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhanu141/Python-programs/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENUei1u1bRpk"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate torch\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# Use your valid Hugging Face token\n",
        "\n",
        "token = os.getenv(\"HF_TOKEN\")\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=token)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, token=token, device_map=\"auto\")\n",
        "\n",
        "# Create generation pipeline\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Prompt for Python code generation'\n",
        "\n",
        "prompt = \"Write a Python function for generating a prime number.\"\n",
        "\n",
        "# Generate output\n",
        "output = generator(prompt, max_length=256, do_sample=True, temperature=0.7, truncation=True)\n",
        "\n",
        "# Print result\n",
        "print(\"Generated Python Code:\\n\", output[0]['generated_text'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§  How to Use This Notebook\n",
        "\n",
        "This notebook generates Python code using the LLaMA model via Hugging Face.\n",
        "\n",
        "### ðŸªª Step 1: Generate a Hugging Face Token\n",
        "- Go to: https://huggingface.co/settings/tokens\n",
        "- Click **\"New token\"**\n",
        "- Name it, set **Scope = Read**, and click **Create**\n",
        "- Copy the token\n",
        "\n",
        "### ðŸ§ª Step 2: Add Your Token\n",
        "Paste this before running any code block:\n",
        "\n",
        "```python\n",
        "import os\n",
        "os.environ[\"HF_TOKEN\"] = \"your_token_here\"\n"
      ],
      "metadata": {
        "id": "9H9_Q_iObVqe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Install Required Libraries (if not already installed)\n",
        "python\n",
        "Copy code\n",
        "!pip install transformers torch accelerate\n",
        "\n",
        "Step 4: Run the notebook (GPU recommended)\n",
        "Works perfectly in Google Colab\n",
        "\n",
        "Make sure runtime type is GPU:\n",
        "\n",
        "Runtime > Change runtime type > Select GPU\n",
        "\n",
        "yaml\n",
        "Copy code\n"
      ],
      "metadata": {
        "id": "qrk085qZcSx3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ORBpYVFfcZaz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}